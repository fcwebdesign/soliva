import { NextRequest, NextResponse } from 'next/server';
import { readContent } from '@/lib/content';

const models = ['gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-4o', 'gpt-4o-mini'];

export async function POST(request: NextRequest) {
  try {
    const { model } = await request.json();
    
    if (!models.includes(model)) {
      return NextResponse.json(
        { error: `Mod√®le non support√©: ${model}` },
        { status: 400 }
      );
    }

    const content = await readContent();
    
    // Contexte simple pour le test
    const pageContext = `Page: Studio cr√©atif Soliva. ${content.studio?.hero?.title || 'Le studio'}. ${content.studio?.description || ''}`;

    // D√©terminer le bon param√®tre selon le mod√®le
    const isGpt5 = model.startsWith('gpt-5');
    const requestBody = {
      model: model,
      messages: [
        {
          role: 'system',
          content: `Tu es un copywriter. R√©ponds imm√©diatement en texte final. Pas d'analyse interm√©diaire. Exactement 2-3 phrases.`
        },
        {
          role: 'user',
          content: `√âcris un paragraphe court (2-3 phrases) pour un studio cr√©atif. Style professionnel. R√©ponds directement.`
        }
      ],
    };

    // Utiliser le bon param√®tre selon le mod√®le
    if (isGpt5) {
      // GPT-5 : R√©duire drastiquement l'effort de raisonnement
      requestBody.max_completion_tokens = 200;
      requestBody.reasoning_effort = "minimal";
      console.log('üîß Configuration GPT-5:', JSON.stringify(requestBody, null, 2));
    } else {
      requestBody.max_tokens = 100;
      requestBody.temperature = 0.7;
      console.log('üîß Configuration GPT-4:', JSON.stringify(requestBody, null, 2));
    }

    console.log('üöÄ Envoi requ√™te OpenAI pour mod√®le:', model);
    const openaiResponse = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestBody),
    });
    
    console.log('üì° Statut r√©ponse OpenAI:', openaiResponse.status, openaiResponse.statusText);

    if (!openaiResponse.ok) {
      const errorText = await openaiResponse.text();
      console.error('‚ùå Erreur OpenAI:', openaiResponse.status, errorText);
      return NextResponse.json(
        { error: `OpenAI API error: ${openaiResponse.status} - ${errorText}` },
        { status: openaiResponse.status }
      );
    }

    const openaiData = await openaiResponse.json();
    console.log('OpenAI Response:', JSON.stringify(openaiData, null, 2));
    
    const suggestedContent = openaiData.choices?.[0]?.message?.content;

    if (!suggestedContent) {
      console.error('Pas de contenu dans la r√©ponse OpenAI:', openaiData);
      throw new Error('Pas de r√©ponse de l\'IA');
    }

    return NextResponse.json({
      model: model,
      suggestedContent: suggestedContent.trim(),
      status: 'success'
    });

  } catch (error) {
    console.error('Erreur test mod√®le:', error);
    return NextResponse.json(
      { error: error.message || 'Erreur lors du test' },
      { status: 500 }
    );
  }
} 